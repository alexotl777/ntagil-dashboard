import base64
import io
import time

from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from bertopic import BERTopic
from loguru import logger
import nltk
from nltk.corpus import stopwords
import re
import requests


# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text.lower())
    return text


class Clustering:

    def __init__(self, lemma_df):
        nltk.download('stopwords')
        self.russian_stopwords = stopwords.words('russian')
        self.topic_model = None

        self.url = "https://api.deepinfra.com/v1/openai/chat/completions"

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        self.df = lemma_df[lemma_df['label'] == 'negative']
        self.df = self.df.dropna(subset=['text_preprocessed'])

        self.df['cleaned'] = self.df['text_preprocessed'].apply(clean_text)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        vectorizer = TfidfVectorizer(stop_words=self.russian_stopwords, max_df=0.9, min_df=10)
        self.X_tfidf = vectorizer.fit_transform(self.df['cleaned'])

        self.topics = []

    def fit_bert_topic(self):
        # –û–±—É—á–µ–Ω–∏–µ BERTopic
        texts = self.df['cleaned'].tolist()
        logger.info(f"Texts count - {len(texts)}. Clustering...")
        self.topic_model = BERTopic(language="multilingual", calculate_probabilities=True, verbose=True)
        topics, probs = self.topic_model.fit_transform(texts)
        logger.info("Clustering done!")
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º—ã –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        self.df['topic'] = topics

    def get_topics(self):

        # –ü–æ–∫–∞–∑ —Ç–µ–º
        logger.info("\nüß† –¢–æ–ø-15 —Ç–µ–º:")
        for idx, topic in self.topic_model.get_topics().items():
            if idx == -1:
                continue  # -1 –æ–∑–Ω–∞—á–∞–µ—Ç "—à—É–º"
            logger.info(f"–¢–µ–º–∞ {idx}: " + " | ".join([word for word, _ in topic[:10]]))
            self.topics.append(topic)
            if idx >= 45:
                break
        return self.topics

    def get_topics_names(self):
        result = []
        for idx, topic in self.topic_model.get_topics().items():
            if idx == -1:
                continue  # -1 –æ–∑–Ω–∞—á–∞–µ—Ç "—à—É–º"
            name = str(self._get_topic_name_openrouter(topic, idx))
            self.df.loc[self.df.topic == idx, "topic_name"] = name
            result.append(name)
            time.sleep(0.8)
            if idx >= 45:
                break
        return result

    def _get_topic_name(self, words: list, number: int):
        headers = {
            "Authorization": "Bearer Hqml68jmyOwWpG85tKfE3bYSEhisuh7H",  # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è: https://deepinfra.com/
            "Content-Type": "application/json"
        }
        data = {
            # "model": "mistralai/Mistral-7B-Instruct-v0.1",
            "model": "meta-llama/Meta-Llama-3-70B-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": f"–ò—Å–ø–æ–ª—å–∑—É—è –æ–±–ª–∞–∫–æ —Å–ª–æ–≤, –æ–ø—Ä–µ–¥–µ–ª–∏ –∫—Ä–∞—Ç–∫–æ –æ–±—â—É—é —Ç–µ–º—É –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –∂–∞–ª—É—é—Ç—Å—è –ª—é–¥–∏, –Ω–∞–ø–∏—à–∏ –≤ –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ—Ä–∞–∑–µ –ø–æ-—Ä—É—Å—Å–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '—Ç–µ–º–∞: (–Ω–µ –±–æ–ª–µ–µ 4 —Å–ª–æ–≤ –ø–æ —Ç–µ–º–µ)': {words[:30]}"
                }
            ]
        }

        response = requests.post(self.url, json=data, headers=headers)
        try:
            logger.info(f"Topic {number}: {response.json()["choices"][0]["message"]["content"]}")
            return response.json()["choices"][0]["message"]["content"]
        except:
            logger.error(f"Incorrect resp: {response.json()}")
            raise

    def _get_topic_name_openrouter(self, words: list, number: int):
        # https://openrouter.ai/
        url = "https://openrouter.ai/api/v1/completions"

        payload = {
            "model": "google/gemma-3-27b-it:free",
            "prompt": f"–ò—Å–ø–æ–ª—å–∑—É—è –æ–±–ª–∞–∫–æ —Å–ª–æ–≤, –æ–ø—Ä–µ–¥–µ–ª–∏ –∫—Ä–∞—Ç–∫–æ –æ–±—â—É—é —Ç–µ–º—É –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –∂–∞–ª—É—é—Ç—Å—è –ª—é–¥–∏, –≤ –æ–¥–Ω–æ–º –∫–æ—Ä–æ—Ç–∫–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏: {words[:30]}"
        }
        headers = {
            "Authorization": "Bearer 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",
            "Content-Type": "application/json"
        }

        response = requests.post(url, json=payload, headers=headers)
        try:
            logger.info(f"Topic {number}: {response.json()["choices"][0]["text"]}")
            return response.json()["choices"][0]["text"]
        except:
            logger.error(f"Incorrect resp: {response.json()}")
            raise

    def _get_topic_name_hugging(self, words: list, number: int):
        headers = {
            "Authorization": "Bearer Hqml68jmyOwWpG85tKfE3bYSEhisuh7H",  # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è: https://deepinfra.com/
            "Content-Type": "application/json"
        }
        data = {
            # "model": "mistralai/Mistral-7B-Instruct-v0.1",
            "model": "meta-llama/Meta-Llama-3-70B-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": f"–ò—Å–ø–æ–ª—å–∑—É—è –æ–±–ª–∞–∫–æ —Å–ª–æ–≤, –æ–ø—Ä–µ–¥–µ–ª–∏ –∫—Ä–∞—Ç–∫–æ –æ–±—â—É—é —Ç–µ–º—É –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –∂–∞–ª—É—é—Ç—Å—è –ª—é–¥–∏, –Ω–∞–ø–∏—à–∏ –≤ –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ—Ä–∞–∑–µ –ø–æ-—Ä—É—Å—Å–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '—Ç–µ–º–∞: (–Ω–µ –±–æ–ª–µ–µ 4 —Å–ª–æ–≤ –ø–æ —Ç–µ–º–µ)': {words[:30]}"
                }
            ]
        }

        response = requests.post(self.url, json=data, headers=headers)
        try:
            logger.info(f"Topic {number}: {response.json()["choices"][0]["message"]["content"]}")
            return response.json()["choices"][0]["message"]["content"]
        except:
            logger.error(f"Incorrect resp: {response.json()}")
            raise

    def get_word_clouds(self):
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–ª–∞–∫–æ–≤ —Å–ª–æ–≤ –ø–æ —Ç–µ–º–∞–º
        unique_topics = self.df['topic'].unique()
        for topic_id in sorted(unique_topics):
            if topic_id == -1:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º
            text = " ".join(self.df[self.df['topic'] == topic_id]['cleaned'])
            wordcloud = WordCloud(width=800, height=400, background_color='white',
                                  stopwords=self.russian_stopwords).generate(text)
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(f'–û–±–ª–∞–∫–æ —Å–ª–æ–≤ –¥–ª—è —Ç–µ–º—ã {topic_id}')
            plt.show()

    def get_wordcloud_base64_by_topic(self, topic_id):
        logger.info(f"Generation WordCloud for: {topic_id}")
        print(self.df['topic'].unique())
        if topic_id not in self.df['topic'].unique():
            return None
        text = " ".join(self.df[self.df['topic'] == topic_id]['cleaned'])
        wordcloud = WordCloud(width=800, height=400, background_color='white',
                              stopwords=self.russian_stopwords).generate(text)
        buffer = io.BytesIO()
        wordcloud.to_image().save(buffer, format='PNG')
        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return f"data:image/png;base64,{img_base64}"

    def get_comments_by_topic(self, topic_id, n=20):
        return self.df[self.df['topic'] == topic_id]['text'].head(n).tolist()


class WordCloudShower:
    def __init__(self, df):
        self.df = df.dropna(subset=['topic_name'])
        nltk.download('stopwords')
        self.russian_stopwords = stopwords.words('russian')

    def get_topic_names(self):
        return sorted(list(self.df['topic_name'].unique()))

    def get_wordcloud_base64_by_topic(self, topic_id):
        logger.info(f"Generation WordCloud for: {topic_id}")
        if topic_id not in self.df['topic_name'].unique():
            return None
        text = " ".join(self.df[self.df['topic_name'] == topic_id]['cleaned'])
        wordcloud = WordCloud(width=800, height=400, background_color='white',
                              stopwords=self.russian_stopwords).generate(text)
        buffer = io.BytesIO()
        wordcloud.to_image().save(buffer, format='PNG')
        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return f"data:image/png;base64,{img_base64}"

    def get_comments_by_topic(self, topic_id, n=20):
        return self.df[self.df['topic_name'] == topic_id]['text'].head(n).tolist()
